{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.clustering import KMeans as KMeansSpark\n",
    "from pyspark.sql.functions import col\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.linalg import SparseVector\n",
    "from pyspark.sql.types import DoubleType\n",
    "from minepy import MINE\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "\n",
    "\n",
    "# Define the KMeans class\n",
    "class KMeans():\n",
    "    def _init_(self, k=5, max_iterations=500):\n",
    "        self.k = k\n",
    "        self.max_iterations = max_iterations\n",
    "        self.kmeans_centroids = None\n",
    "\n",
    "\n",
    "    def _init_random_centroids(self, data):\n",
    "        centroids = data.sample(False, 1.0).limit(self.k).collect()\n",
    "        return [row[\"features\"] for row in centroids]\n",
    "\n",
    "    def _closest_centroid(self, sample, centroids):\n",
    "        closest_i = None\n",
    "        closest_score = float(\"inf\")\n",
    "        for i, centroid in enumerate(centroids):\n",
    "            if isinstance(sample, SparseVector):\n",
    "                distance = np.sqrt(sample.squared_distance(centroid))\n",
    "                score = distance\n",
    "            else:\n",
    "                distance = np.linalg.norm(sample - centroid)\n",
    "                score = distance\n",
    "            if score < closest_score:\n",
    "                closest_i = i\n",
    "                closest_score = score\n",
    "        return closest_i\n",
    "\n",
    "    def _create_clusters(self, centroids, data):\n",
    "        clusters = [[] for _ in range(self.k)]\n",
    "        for row in data.collect():\n",
    "            sample = row[\"features\"]\n",
    "            centroid_i = self._closest_centroid(sample, centroids)\n",
    "            clusters[centroid_i].append(row)\n",
    "        return clusters\n",
    "\n",
    "    def _calculate_centroids(self, clusters, data):\n",
    "        centroids = []\n",
    "        for cluster in clusters:\n",
    "            if cluster:\n",
    "                cluster_points = [row[\"features\"] for row in cluster]\n",
    "                cluster_mean = np.mean(cluster_points, axis=0)\n",
    "                centroids.append(cluster_mean)\n",
    "            else:\n",
    "                # If the cluster is empty, select a random point from the dataset as the centroid\n",
    "                random_row = np.random.randint(0, data.count())\n",
    "                centroids.append(data.collect()[random_row][\"features\"])\n",
    "        return np.array(centroids)\n",
    "\n",
    "    def fit(self, data):\n",
    "        centroids = self._init_random_centroids(data)\n",
    "        for _ in range(self.max_iterations):\n",
    "            clusters = self._create_clusters(centroids, data)\n",
    "            new_centroids = self._calculate_centroids(clusters, data)\n",
    "\n",
    "            # Check convergence\n",
    "            if np.allclose(new_centroids, centroids):\n",
    "                self.kmeans_centroids = new_centroids\n",
    "                return new_centroids\n",
    "\n",
    "            centroids = new_centroids\n",
    "\n",
    "        self.kmeans_centroids = centroids\n",
    "        return centroids\n",
    "\n",
    "    def predict(self, data):\n",
    "        if self.kmeans_centroids is None:\n",
    "            raise Exception(\"Run the fit function first\")\n",
    "\n",
    "        centroids = self.kmeans_centroids\n",
    "        clusters = self._create_clusters(centroids, data)\n",
    "        predicted_labels = []\n",
    "        for i, cluster in enumerate(clusters):\n",
    "            for row in cluster:\n",
    "                predicted_labels.append((i, row[\"features\"]))\n",
    "        return predicted_labels\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KMeans with PySpark\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load CSV data into DataFrame\n",
    "df = spark.read.csv(\"./adult.csv\", header=True, inferSchema=True) #As we have used a adult/universal data set for example , instead of this we can use the unstop dataset or any other to run the K means Cluster alogorithm.\n",
    "\n",
    "# Updated column names\n",
    "column_names = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation',\n",
    "         'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income']\n",
    "\n",
    "# Rename the columns\n",
    "df = df.toDF(*column_names)\n",
    "\n",
    "# Drop rows with missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Define categorical features\n",
    "categorical_features = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\n",
    "\n",
    "# Index categorical columns\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\", handleInvalid=\"keep\") for column in categorical_features]\n",
    "indexers_pipeline = Pipeline(stages=indexers)\n",
    "indexed_data = indexers_pipeline.fit(df).transform(df)\n",
    "\n",
    "# Assemble features into a single vector\n",
    "assembler = VectorAssembler(inputCols=['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week'] + [column+\"_index\" for column in categorical_features], outputCol=\"features\")\n",
    "assembled_data = assembler.transform(indexed_data)\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=False)\n",
    "scaler_model = scaler.fit(assembled_data)\n",
    "scaled_data = scaler_model.transform(assembled_data)\n",
    "\n",
    "\n",
    "kmeans_custom = KMeans(k=5)\n",
    "\n",
    "# Fit the data\n",
    "kmeans_custom.fit(scaled_data)\n",
    "\n",
    "# Predict labels\n",
    "predicted_labels = kmeans_custom.predict(scaled_data)\n",
    "\n",
    "print(\"Predicted Labels:\")\n",
    "for label, _ in predicted_labels:\n",
    "    print(label)\n",
    "\n",
    "from pyspark.ml.feature import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Print clusters\n",
    "print(\"Clusters:\")\n",
    "for i, cluster in enumerate(kmeans_custom.kmeans_centroids):\n",
    "    print(f\"Cluster {i}: {cluster}\")\n",
    "\n",
    "# Apply PCA\n",
    "num_dimensions = 2  # Set the number of dimensions for PCA\n",
    "pca = PCA(k=num_dimensions, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "model = pca.fit(scaled_data)\n",
    "result = model.transform(scaled_data)\n",
    "\n",
    "# Extract PCA components for plotting\n",
    "pca_data = result.select(\"pca_features\").rdd.map(lambda x: x.pca_features).collect()\n",
    "x_values_pca = [point[0] for point in pca_data]\n",
    "y_values_pca = [point[1] for point in pca_data]\n",
    "\n",
    "# Plot clusters using PCA components\n",
    "plt.scatter(x_values_pca, y_values_pca, c=x_values_pca, cmap='viridis')\n",
    "plt.title('KMeans Clustering with PCA')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
